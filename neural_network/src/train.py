#!/usr/bin/env python3

"""
Anti-Stockfish: Optimized Training Script for Apple M4 Pro
- Metal Performance Shaders (MPS) GPU acceleration
- Streaming Dataset (IterableDataset) for low memory usage
- Large batch sizes (1024+) for max throughput
- Optimized for Apple Silicon
- Supports Checkpointing & Resuming
- Supports Run.Epoch Versioning (e.g., V1.3)
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import IterableDataset, DataLoader
import json
import logging
import argparse
from pathlib import Path
import sys
import itertools
import os

# Import the model
sys.path.append(str(Path(__file__).parent))
from model import ChaosModule, SacrificeModule, board_to_tensor

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class StreamingChessDataset(IterableDataset):
    """Memory-efficient streaming dataset for large files."""
    
    def __init__(self, data_file):
        self.data_file = data_file
        
    def __iter__(self):
        """Yields batches of data directly from file without loading everything to RAM."""
        with open(self.data_file) as f:
            for line in f:
                try:
                    pos = json.loads(line)
                    
                    # Convert position to tensor
                    board_tensor = board_to_tensor(pos['fen'])
                    
                    # Labels
                    label_map = {'normal': 0, 'complex': 1, 'sacrifice': 2}
                    label = label_map.get(pos.get('label', 'normal'), 0)
                    
                    # Evaluation (normalized)
                    eval_score = pos.get('eval', 0.0)
                    eval_normalized = max(-1.0, min(1.0, eval_score / 10.0))
                    
                    yield {
                        'board': board_tensor,
                        'label': torch.tensor(label, dtype=torch.long),
                        'eval': torch.tensor(eval_normalized, dtype=torch.float32),
                        'chaos': torch.tensor(1.0 if label == 2 else 0.5 if label == 1 else 0.0, dtype=torch.float32)
                    }
                except:
                    continue

def train_epoch(chaos_model, sacrifice_model, dataloader, optimizer_chaos, optimizer_sac, device, total_batches):
    """Train for one epoch."""
    chaos_model.train()
    sacrifice_model.train()
    
    total_loss = 0.0
    num_batches = 0
    
    for batch_idx, batch in enumerate(dataloader):
        board = batch['board'].to(device)
        label = batch['label'].to(device)
        eval_target = batch['eval'].to(device)
        chaos_target = batch['chaos'].to(device)
        
        # Chaos Module forward
        optimizer_chaos.zero_grad()
        policy, value, chaos = chaos_model(board)
        
        # Chaos loss
        policy_loss = nn.CrossEntropyLoss()(policy, label)
        value_loss = nn.MSELoss()(value.squeeze(), eval_target)
        chaos_loss = nn.MSELoss()(chaos.squeeze(), chaos_target)
        
        total_chaos_loss = policy_loss + value_loss + chaos_loss
        total_chaos_loss.backward()
        optimizer_chaos.step()
        
        # Sacrifice Module forward
        optimizer_sac.zero_grad()
        sacrifice_prob = sacrifice_model(board)
        sacrifice_target = (label == 2).float()
        sacrifice_loss = nn.BCELoss()(sacrifice_prob.squeeze(), sacrifice_target)
        sacrifice_loss.backward()
        optimizer_sac.step()
        
        total_loss += (total_chaos_loss.item() + sacrifice_loss.item())
        num_batches += 1
        
        if (batch_idx + 1) % 100 == 0:
            avg_loss = total_loss / num_batches
            percent = ((batch_idx + 1) / total_batches) * 100
            # Cap percentage at 100% for display purposes if estimation was off
            if percent > 100:
                percent_str = ">100%"
            else:
                percent_str = f"{percent:.1f}%"
                
            logger.info(f"  Batch {batch_idx + 1}/{total_batches} ({percent_str}) | Loss: {avg_loss:.8f}")
    
    return total_loss / max(1, num_batches)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data', required=True, help='Path to dataset')
    parser.add_argument('--epochs', type=int, default=5, help='Number of epochs')
    parser.add_argument('--batch-size', type=int, default=1024, help='Batch size')
    parser.add_argument('--device', default='mps', help='Device (mps, cuda, cpu)')
    parser.add_argument('--num-workers', type=int, default=4, help='DataLoader workers')
    parser.add_argument('--resume', action='store_true', help='Resume from latest checkpoint if available')
    parser.add_argument('--run-id', type=int, default=1, help='Run ID for versioning (e.g. Run 1)')
    args = parser.parse_args()
    
    # Device setup
    if args.device == 'mps' and torch.backends.mps.is_available():
        device = torch.device('mps')
        logger.info("ðŸ”¥ Using Metal Performance Shaders (MPS) GPU!")
    elif args.device == 'cuda' and torch.cuda.is_available():
        device = torch.device('cuda')
        logger.info("ðŸ”¥ Using CUDA GPU!")
    else:
        device = torch.device('cpu')
        logger.info("ðŸ’» Using CPU")
    
    logger.info(f"âš™ï¸  Batch size: {args.batch_size}")
    logger.info(f"âš™ï¸  Workers: {args.num_workers}")
    logger.info(f"âš™ï¸  Epochs: {args.epochs}")
    logger.info(f"ðŸ·ï¸  Run ID: {args.run_id}")
    
    # Load dataset (Streaming)
    dataset = StreamingChessDataset(args.data)
    
    # Calculate total batches (approximate based on file line count)
    total_lines = 0
    try:
        with open(args.data) as f:
            for _ in f: total_lines += 1
    except:
        total_lines = 1000000 # Fallback
        
    total_batches = total_lines // args.batch_size
    logger.info(f"ðŸ“Š Total positions: {total_lines:,}")
    logger.info(f"ðŸ“¦ Total batches per epoch: {total_batches:,}")

    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        pin_memory=False  # Disabled for MPS stability
    )
    
    # Models
    chaos_model = ChaosModule().to(device)
    sacrifice_model = SacrificeModule().to(device)
    
    # Optimizers
    optimizer_chaos = optim.Adam(chaos_model.parameters(), lr=0.001)
    optimizer_sac = optim.Adam(sacrifice_model.parameters(), lr=0.001)
    
    # Checkpoint directory
    model_dir = Path("neural_network/models")
    model_dir.mkdir(parents=True, exist_ok=True)
    
    # Base paths for latest model (always overwritten to allow resuming)
    chaos_path_latest = model_dir / "chaos_module_latest.pth"
    sac_path_latest = model_dir / "sacrifice_module_latest.pth"
    
    # Resume logic
    start_epoch = 0
    if args.resume:
        if chaos_path_latest.exists() and sac_path_latest.exists():
            logger.info("ðŸ”„ Found existing models. Resuming training...")
            try:
                chaos_model.load_state_dict(torch.load(chaos_path_latest, map_location=device))
                sacrifice_model.load_state_dict(torch.load(sac_path_latest, map_location=device))
                logger.info("âœ… Models loaded successfully!")
            except Exception as e:
                logger.error(f"âš ï¸ Failed to load models: {e}. Starting from scratch.")
        else:
            logger.info("â„¹ï¸  No existing models found. Starting fresh.")
    
    logger.info(f"\n{'='*80}")
    logger.info(f"ðŸ§  TRAINING STARTED (STREAMING MODE)")
    logger.info(f"{'='*80}\n")
    
    # Training loop
    for epoch in range(start_epoch, args.epochs):
        current_epoch = epoch + 1
        version_tag = f"v{args.run_id}.{current_epoch}"
        
        logger.info(f"Epoch {current_epoch}/{args.epochs} (Version {version_tag})")
        
        avg_loss = train_epoch(
            chaos_model,
            sacrifice_model,
            dataloader,
            optimizer_chaos,
            optimizer_sac,
            device,
            total_batches
        )
        
        logger.info(f"âœ… Epoch {current_epoch} complete! Avg Loss: {avg_loss:.8f}")
        
        # 1. Save "Latest" for resuming
        torch.save(chaos_model.state_dict(), chaos_path_latest)
        torch.save(sacrifice_model.state_dict(), sac_path_latest)
        
        # 2. Save Versioned Checkpoint (e.g., chaos_module_v1.3.pth)
        chaos_path_ver = model_dir / f"chaos_module_{version_tag}.pth"
        sac_path_ver = model_dir / f"sacrifice_module_{version_tag}.pth"
        torch.save(chaos_model.state_dict(), chaos_path_ver)
        torch.save(sacrifice_model.state_dict(), sac_path_ver)
        
        logger.info(f"ðŸ’¾ Checkpoint saved: {version_tag}")
        logger.info(f"   -> {chaos_path_ver}")
    
    logger.info(f"\n{'='*80}")
    logger.info(f"âœ… TRAINING COMPLETE!")
    logger.info(f"{'='*80}\n")

if __name__ == '__main__':
    main()
